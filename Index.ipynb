{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "\\newcommand{\\advseq}{\\mathbb{A}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Classical Machine Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "<html>\n",
    "<p style=\"font-size:26px\"><strong>Week 0</strong></p>\n",
    "</html>\n",
    " \n",
    "\n",
    "**Plan**\n",
    "- Setting up your learning and programming environment\n",
    "\n",
    "\n",
    "**Getting started**\n",
    "- [Setting up your ML environment](Setup_NYU.ipynb)\n",
    "    - [Choosing an ML environment](Choosing_an_ML_Environment_NYU.ipynb)\n",
    "- [Quick intro to the tools](Getting_Started.ipynb)\n",
    "\n",
    "<!--- #include (README.md) --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 1: Introduction\n",
    "**Plan**\n",
    "- Motivate Machine Learning\n",
    "- Introduce notation used throughout course\n",
    "- Plan for initial lectures\n",
    "    - *What*: Introduce, motivate a model\n",
    "    - *How*:  How to use a model: function signature, code (API)\n",
    "    - *Why*:  Mathematical basis -- enhance understanding and ability to improve results\n",
    "\n",
    "        \n",
    "- [Course Overview](Course_overview_NYU.ipynb)\n",
    "- [Machine Learning: Overview](ML_Overview.ipynb)\n",
    "- [Intro to Classical ML](Intro_Classical_ML.ipynb)\n",
    "\n",
    "## Using an AI Assistant\n",
    "\n",
    "AI Assistants are often very good at coding.\n",
    "\n",
    "But using one to just \"get the answer\" deprives you of a valuable tool\n",
    "- you can ask the Assistant *why* it chose to do something\n",
    "- keep on asking\n",
    "- treat it as a private tutor !\n",
    "\n",
    "[Learning about the Landscape of ML](https://www.perplexity.ai/search/i-am-interested-in-the-landsca-_yO63NWfSGS8iHR5nyQYVA)\n",
    "\n",
    "[Learning about KNN using an Assistant as a private tutor](https://www.perplexity.ai/search/using-python-and-sklearn-pleas-407oe3uzTXu1i9xEHVR2MQ)\n",
    "- [Code answer from Assistant](KNN_illustration_Perplexity.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recipe for Machine Learning\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We will learn the Recipe for Machine Learning,  a disciplined approach to solving problems in Machine Learning.\n",
    "\n",
    "We will illustrate the Recipe while, at the same time,\n",
    "introducing a model for the Regression task: Linear Regression.\n",
    "\n",
    "Our coverage of the Recipe will be rapid and shallow (we use an extremely simple example for illustration).\n",
    "\n",
    "\n",
    "[The Recipe for Machine Learning: Solving a Regression task](Recipe_via_Linear_Regression.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2: Recipe for Machine Learning/Regression task (continued)\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We continue learning the Recipe via a highly simplified Regression task.\n",
    "\n",
    "I highly recommend reviewing and understanding\n",
    "this [Geron notebook](external/handson-ml2/02_end_to_end_machine_learning_project.ipynb)\n",
    "in order to acquire a more in-depth appreciation of the Recipe.\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Recipe for Machine Learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/W1_L3_S4_ML_Process.png\" width=\"100%\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "**Recipe, as illustrated by Linear Regression**\n",
    "\n",
    "[The Recipe for Machine Learning: Solving a Regression task (continued)](Recipe_via_Linear_Regression.ipynb#Error-Analysis)\n",
    "     \n",
    "**Fitting a model: details**\n",
    "\n",
    "Recall: fitting a model (finding optimal value for the parameters) is found by minimizing a Loss function.\n",
    "\n",
    "Let's examine a typical Loss function for Regression\n",
    "- [Regression: Loss Function](Linear_Regression_Loss_Function.ipynb)\n",
    "\n",
    "**Iterative training: when to stop**\n",
    "\n",
    "Increasing the number of parameters of a model improves in-sample fit (reduces Loss) but may compromise\n",
    "out-of-sample prediction (generalization).\n",
    "\n",
    "We examine the issues of having too many/too few parameters.\n",
    "- [When to stop iterating: Bias and Variance](Bias_and_Variance.ipynb)\n",
    "\n",
    "**Get the data: Fundamental Assumption of Machine Learning**\n",
    "\n",
    "- [Getting *good* training examples](Recipe_Training_data.ipynb)\n",
    "\n",
    "**Regression: final thoughts (for now)**\n",
    "\n",
    "- [Regression: coda](Regression_coda.ipynb)\n",
    "\n",
    "**Deeper dives**\n",
    "- [Fine tuning techniques](Fine_tuning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recipe \"Prepare the Data\" step: Transformations\n",
    "\n",
    "We discuss the importance of adding *synthetic* features to our Linear Regression example\n",
    "- and *preview* the *mechanical* process of creating these features via *Transformations*\n",
    "\n",
    "**Transformations**\n",
    " - [Prepare Data: Intro to Transformations](Prepare_data_Overview.ipynb)\n",
    "\n",
    "## Validation\n",
    "\n",
    "Our test dataset can be used only once, yet\n",
    "- we have an iterative process for developing models\n",
    "- each iteration requires a proxy for out of sample data to use in the Performance Metric\n",
    "\n",
    "The solution: create a proxy for out of sample that is a *subset* of the training data.\n",
    "\n",
    "\n",
    "- [Validation and Cross-Validation](Recipe_via_Linear_Regression.ipynb#Validation-and-Cross-Validation)\n",
    "- [Avoiding cheating in Cross-Validation](Prepare_data_Overview.ipynb#Using-pipelines-to-avoid-cheating-in-cross-validation)\n",
    "\n",
    "## Using an AI Assistant\n",
    "\n",
    "[Learning about Linear Regression using an Assistant as private tutor](https://www.perplexity.ai/search/using-python-sklearn-and-matpl-vTYy7oGdRQ6upR5L5OSjrg)\n",
    "- [Code Answer from Assistant](LinearRegression_Illustration_Perplexity.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification task\n",
    "\n",
    "**Plan**\n",
    "- We introduce a model for the Classification task: Logistic Regression\n",
    "- How to deal with Categorical (non-numeric) variables\n",
    "    - classification target\n",
    "    - features\n",
    "\n",
    "**Classification intro**\n",
    "- [Classification: Overview](Classification_Overview.ipynb)\n",
    "- [Classification and Categorical Variables](Classification_and_Non_Numerical_Data.ipynb)\n",
    "    - [linked notebook](Classification_and_Non_Numerical_Data.ipynb)\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 3: Classification task\n",
    "\n",
    "**Classification taks (continued)**\n",
    "\n",
    "- [Classification and Categorical Variables (continued)](Classification_and_Non_Numerical_Data.ipynb#Recipe-Step-B:-Exploratory-Data-Analysis-(EDA))\n",
    "\n",
    "**Categorical variables** (contained as subsections of Classification and Categorical Variables)\n",
    "\n",
    "We examine the proper treatment of categorical variables (target or feature).\n",
    "\n",
    "Along the way, we run into a subtle difficulty: the Dummy Variable Trap.\n",
    "\n",
    "- [Classification and Categorical Variables: Categorical Variables](Classification_Notebook_Overview.ipynb#Categorical-variables)\n",
    "    - [Categorical variables, One Hot Encoding (OHE)](Categorical_Variables.ipynb)\n",
    "\n",
    "**Multinomial Classification**\n",
    "\n",
    "We generalize Binary Classification into classification into more than two classes.\n",
    "\n",
    "- [Multinomial Classification](Multinomial_Classification.ipynb)\n",
    "\n",
    "**Error Analysis**\n",
    "\n",
    "We can only improve our model's out of sample Performance Metric\n",
    "- by diagnosing the in-sample errors\n",
    "- that is the goal of the Error Analysis step of the Recipe\n",
    "- We explain Error Analysis for the Classification Task, with a detailed example\n",
    "- How Training Loss can be improved\n",
    "\n",
    "The conversion of a probability (e.g., model output) to a Class (categorical variable) for Classification\n",
    "- often involves the comparison of a probability to a threshold\n",
    "- we show how varying the threshold changes the conditional Performance Metric for Classification\n",
    "    - the threshold is a hyper-parameter, thus this is a kind of Fine-Tuning\n",
    "  \n",
    "- [Error Analysis](Error_Analysis_Overview.ipynb)\n",
    "    - [linked notebook](Error_Analysis.ipynb)\n",
    "        - Summary statistics\n",
    "        - Conditional statistics\n",
    "    - [Worked example](Error_Analysis_MNIST.ipynb)( **Defer to next week ??** )\n",
    "\n",
    "- [Loss Analysis: Using training loss to improve models](Training_Loss.ipynb)\n",
    "\n",
    "**Classification and Categorical variables wrapup**\n",
    "\n",
    "- [Classification Loss Function](Classification_Loss_Function.ipynb)\n",
    "- [Baseline model for Classification](Classification_Baseline_Model.ipynb)\n",
    "- [OHE issue: Dummy variable trap](Dummy_Variable_Trap.ipynb)\n",
    "\n",
    "\n",
    "**Classification: final thoughts (for now)**\n",
    "\n",
    "- [Classification: coda](Classification_coda.ipynb)\n",
    "\n",
    "\n",
    "**Deeper dives**\n",
    "- [Log odds](Classification_Log_Odds.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 4: Transformations\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Now you know how to create models.  What happens if the Performance Metric for your model\n",
    "is disappointing ?\n",
    "\n",
    "The first step is recognizing the issue, and diagnosing it.  That is the role of Error Analysis.\n",
    "\n",
    "The second step is attempting to improve Performance.  Quite often we will need to perform\n",
    "*Feature Engineering*.\n",
    "\n",
    "We explain\n",
    "- why it is often necessary to create *synthetic* features to augment or replace *raw* feature\n",
    "- the mechanical process in `sklearn` that makes the application of transformations easy and consistent\n",
    "\n",
    "## Transformations: the \"why\"\n",
    "\n",
    "Part of becoming a better Data Scientist is transforming raw features into more useful synthetic features.\n",
    "\n",
    "We focus\n",
    "on the necessity (the \"why\"): transforming raw data into something that tells a story.\n",
    "\n",
    "We will then discuss the mechanics (how to use `sklearn` to implement transformation Pipelines) of Transformations.\n",
    "\n",
    "\n",
    "\n",
    "<!--- #include (nvda_normalization_data.csv) --->\n",
    "<!--- #include (MORTGAGE30US.csv) --->\n",
    "\n",
    "- [Becoming a successful Data Scientist](Becoming_a_successful_Data_Scientist.ipynb)\n",
    "- [Transformations: overview](Transformations_Overview.ipynb)\n",
    "    - linked notebooks:\n",
    "        - [Transformations: adding a missing feature](Transformations_Missing_Features.ipynb)\n",
    "\n",
    "\n",
    "**Transformatons: the \"how\" --- Review**\n",
    "\n",
    "We have previously learned \"how\" to implement transformations in `sklearn`.\n",
    "\n",
    "For your review, here is the module on the *mechanical* process of creating transformations\n",
    "*Transformations**\n",
    " - [Prepare Data: Intro to Transformations](Prepare_data_Overview.ipynb)\n",
    "\n",
    "## Imbalanced data\n",
    "- [Imbalanced data](Imbalanced_Data.ipynb)\n",
    "\n",
    "## More models for classification (start)\n",
    "\n",
    "**Plan**\n",
    "- More models: Decision Trees, Naive Bayes, Support Vector Classifier\n",
    "    - Different flavor: more procedural, less mathematical\n",
    "    - Decision Trees: a model with *non-linear* boundaries\n",
    "- Ensembles\n",
    "    - Bagging and Boosting\n",
    "    - Random Forests\n",
    "\n",
    "**Decision Trees, Ensembles**\n",
    "\n",
    "- [Decision Trees: Overview](Decision_Trees_Overview.ipynb)\n",
    "- [Decision Trees](Decision_Trees_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Decision_Trees.ipynb)\n",
    "- [Trees, Forests, Ensembles](Ensembles.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5: More models for classification (continued)\n",
    "\n",
    "## More models for classification\n",
    "\n",
    "**Plan**\n",
    "- More models: Decision Trees, Naive Bayes, Support Vector Classifier\n",
    "    - Different flavor: more procedural, less mathematical\n",
    "    - Decision Trees: a model with *non-linear* boundaries\n",
    "- Ensembles\n",
    "    - Bagging and Boosting\n",
    "    - Random Forests\n",
    "\n",
    "**Decision Trees, Ensembles**\n",
    "\n",
    "- [Decision Trees: Overview](Decision_Trees_Overview.ipynb)  **covered in previous lecture**\n",
    "- [Decision Trees](Decision_Trees_Notebook_Overview.ipynb)   **covered in previous lecture**\n",
    "    - [linked notebook](Decision_Trees.ipynb)\n",
    "- [Trees, Forests, Ensembles](Ensembles.ipynb)  **continued**\n",
    "\n",
    "\n",
    "**Naive Bayes**\n",
    "- [Naive Bayes](Naive_Bayes.ipynb)\n",
    "\n",
    "\n",
    "**Support Vector Classifiers**\n",
    "- [Support Vector Machines: Overview](SVM_Overview.ipynb)\n",
    "- [SVC Loss function](SVM_Hinge_Loss.ipynb)\n",
    "- [SVC: Large Margin Classification](SVM_Large_Margin.ipynb)  \n",
    "- [SVM: Kernel Transformations](SVM_Kernel_Functions.ipynb)\n",
    "- [SVM Wrapup](SVM_Coda.ipynb)\n",
    "\n",
    "**Classification: final thoughts**\n",
    "\n",
    "- [Classification: coda -- review again](Classification_coda.ipynb)\n",
    "\n",
    "\n",
    "## Using an AI Assistant to learn about other models for classification\n",
    "- [SVC conversation](https://www.perplexity.ai/search/what-is-the-relationship-betwe-Pq8r22pISH.gUGmM1gkgbg#4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "**Unsupervised Learning: PCA**\n",
    "- [Unsupervised Learning: Overview](Unsupervised_Overview.ipynb)\n",
    "- [PCA Notebook Overview](Unsupervised_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Unsupervised.ipynb)\n",
    "- [PCA in Finance](PCA_Yield_Curve_Intro.ipynb)\n",
    "\n",
    "**Unsupervised Learning: PCA** (continued)\n",
    "- [Importance of number of components: visualization](Unsupervised.ipynb#Visualizing-the-fidelity-of-the-reduced-dimension-representation)\n",
    "- [Interpreting the components](Unsupervised.ipynb#Can-we-interpret-the-components-?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Week 6: DL Week 1 Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "\n",
    "**Loss functions: mathematical basis** (deferred)\n",
    "\n",
    "Where do the Loss functions of Classical Machine Learning come from ?  We take a brief mathematical\n",
    "detour into Loss functions.\n",
    "\n",
    "- [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
    "- [Loss functions: the math](Loss_functions.ipynb)\n",
    "    - Maximum likelihood\n",
    "    - Preview: custom loss functions and Deep Learning\n",
    "\n",
    "**Deeper Dives**\n",
    "- [Linear Regression in more depth](Linear_Regression_fitting.ipynb)\n",
    "- [Interpretation: Linear Models](Linear_Model_Interpretation.ipynb)\n",
    "- [Missing data: clever ways to impute values](Missing_Data.ipynb)\n",
    "- [Feature importance](Feature_Importance.ipynb)\n",
    "- [SVC Loss function derivation](SVM_Derivation.ipynb)\n",
    "\n",
    "\n",
    "## Bridge between Classical ML and Deep Learning\n",
    "\n",
    "**Gradient Descent** \n",
    "\n",
    "Machine Learning is based on minimization of a Loss Function.  Gradient Descent is one algorithm\n",
    "to achieve that.\n",
    "- [Gradient Descent](Gradient_Descent.ipynb)\n",
    "\n",
    "\n",
    "**Recommender Systems (Pseudo SVD)**\n",
    "\n",
    "How does Amazon/Netflix/etc. recommend products/films to us ?  We describe a method similar to SVD\n",
    "but that is solved using Gradient Descent.\n",
    "\n",
    "This theme of creating a custom Loss Functions and minimizing it via Gradient Descent is a recurring\n",
    "theme in the upcoming Deep Learning second half of the course.\n",
    "\n",
    "- [Recommender Systems](Recommender_Systems.ipynb)\n",
    "- [Preview: Some cool Loss functions](Loss_functions.ipynb#Loss-functions-for-Deep-Learning:-Preview)\n",
    "\n",
    "**Deeper Dives**\n",
    "\n",
    "- [Other matrix factorization methods](Unsupervised_Other_Factorizations.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Additional Deep Learning resources\n",
    "\n",
    "Here are some resources that I have found very useful.\n",
    "\n",
    "Some of them are very nitty-gritty, deep-in-the-weeds (even the \"introductory\" courses)\n",
    "- For example: let's make believe PyTorch (or Keras/TensorFlow) didn't exists; let's invent Deep Learning without it !\n",
    "    - You will gain a deeper appreciation and understanding by re-inventing that which you take for granted\n",
    "    \n",
    "\n",
    "## [Andrej Karpathy course: Neural Networks, Zero to Hero](https://karpathy.ai/zero-to-hero.html)\n",
    "- PyTorch\n",
    "- Introductory, but at a very deep level of understanding\n",
    "    - you will get very deep into the weeds (hand-coding gradients !) but develop a deeper appreciation\n",
    "    \n",
    "## fast.ai\n",
    "\n",
    "`fast.ai` is a web-site with free courses from Jeremy Howard.\n",
    "- PyTorch\n",
    "- Introductory and courses \"for coders\"\n",
    "- Same courses offered every few years, but sufficiently different so as to make it worthwhile to repeat the course !\n",
    "    - [Practical Deep Learning](https://course.fast.ai/)\n",
    "    - [Stable diffusion](https://course.fast.ai/Lessons/part2.html)\n",
    "        - Very detailed, nitty-gritty details (like Karpathy) that will give you a deeper appreciation\n",
    "        \n",
    "## [Stefan Jansen: Machine Learning for Trading](https://github.com/stefan-jansen/machine-learning-for-trading)\n",
    "\n",
    "An excellent github repo with notebooks\n",
    "- using Deep Learning for trading\n",
    "- Keras\n",
    "- many notebooks are cleaner implementations of published models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignments\n",
    "\n",
    "Your assignments should follow the [Assignment Guidelines](assignments/Assignment_Guidelines.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "- Assignment notebook: [Using Machine Learning for Hedging](assignments/Regression%20task/Using_Machine_Learning_for_Hedging.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Regression task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "- Assignment notebook: [Ships in satellite images](assignments/Classification%20task/Ships_in_satellite_images.ipynb#)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Classification task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Midterm Project: Bankruptcy One Year Ahead\n",
    "- Assignment notebook [Bankruptcy One Year Ahead](assignments/bankruptcy_one_yr/Bankruptcy_oya.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Bankruptcy One Year Ahead\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.9 (new)",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
